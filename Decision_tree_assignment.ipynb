{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "question--1-->\n",
        "\n",
        "A Decision Tree is a popular supervised machine learning algorithm used for classification and regression tasks. It is a tree-like model where decisions are made by splitting data based on certain conditions.\n",
        "\n",
        "How Does a Decision Tree Work?\n",
        "A decision tree consists of:\n",
        "\n",
        "Root Node: The starting point, representing the entire dataset.\n",
        "Internal Nodes: Decision points where the dataset is split based on a feature.\n",
        "Branches: Possible outcomes of a decision.\n",
        "Leaf Nodes: The final output or prediction (class label for classification, value for regression).\n",
        "Working of a Decision Tree\n",
        "Select the Best Feature: The algorithm chooses a feature that best splits the data using criteria like Gini Index or Information Gain (for classification) and Mean Squared Error (MSE) (for regression).\n",
        "Split the Data: The dataset is divided into subsets based on the chosen feature’s value.\n",
        "Repeat Process: Steps 1 and 2 are repeated recursively until a stopping condition is met (e.g., all data in a node belong to the same class or a maximum depth is reached).\n",
        "Prediction: For a new input, the tree is traversed from the root to a leaf node, providing a classification label or numerical prediction."
      ],
      "metadata": {
        "id": "sxzFvF8JdHFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7usy4r0UdXkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question --2-->\n",
        "\n",
        "Impurity Measures in Decision Trees\n",
        "Impurity measures help determine how well a dataset is split at each node of a Decision Tree. A pure node contains data points belonging to a single class, while an impure node has mixed classes. The goal is to minimize impurity when splitting the data.\n",
        "\n",
        "Common Impurity Measures:\n",
        "1. Gini Index (Gini Impurity)\n",
        "Measures the probability of misclassifying a randomly chosen element.\n",
        "Formula:\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−∑p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "where\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  is the probability of a class in a node.\n",
        "Range: 0 (pure node) to 0.5 (highest impurity in a binary case).\n",
        "Used in CART (Classification and Regression Trees).\n",
        "2. Entropy (Information Gain)\n",
        "Measures the randomness in a dataset.\n",
        "Formula:\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "𝑝\n",
        "𝑖\n",
        "Entropy=−∑p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        "\n",
        "Range: 0 (pure node) to 1 (maximum impurity in a binary case).\n",
        "Used in ID3 and C4.5 algorithms.\n",
        "Information Gain is calculated as:\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝑃\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        ")\n",
        "−\n",
        "∑\n",
        "(\n",
        "samples in child\n",
        "total samples\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐶\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        ")\n",
        ")\n",
        "IG=Entropy(Parent)−∑(\n",
        "total samples\n",
        "samples in child\n",
        "​\n",
        " ×Entropy(Child))\n",
        "The feature with the highest IG is chosen for splitting.\n",
        "3. Variance (for Regression Trees)\n",
        "Measures the spread of continuous values at a node.\n",
        "Formula:\n",
        "𝑉\n",
        "𝑎\n",
        "𝑟\n",
        "𝑖\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "Variance=\n",
        "N\n",
        "1\n",
        "​\n",
        " ∑(y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "ˉ\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "where\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "  are actual values and\n",
        "𝑦\n",
        "ˉ\n",
        "y\n",
        "ˉ\n",
        "​\n",
        "  is the mean.\n",
        "4. Misclassification Error\n",
        "Used less frequently but represents the proportion of incorrectly classified elements.\n",
        "Formula:\n",
        "𝐸\n",
        "𝑟\n",
        "𝑟\n",
        "𝑜\n",
        "𝑟\n",
        "=\n",
        "1\n",
        "−\n",
        "max\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Error=1−max(p\n",
        "i\n",
        "​\n",
        " )\n",
        "where\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  is the probability of the most frequent class.\n",
        "Which Impurity Measure is Best?\n",
        "Gini Index is computationally faster and preferred in CART.\n",
        "Entropy is used when interpretability is needed (more sensitive to changes).\n",
        "Variance is used for Regression Trees"
      ],
      "metadata": {
        "id": "Z6ZamMA-dg1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question--3-->\n",
        "\n",
        "The mathematical formula for Gini Impurity is:\n",
        "\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝑐\n",
        "c is the total number of classes.\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  is the probability of a data point belonging to class\n",
        "𝑖\n",
        "i within a node.\n",
        "Explanation:\n",
        "If a node is pure (all samples belong to one class),\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "0\n",
        "Gini=0.\n",
        "If the classes are evenly split, impurity increases.\n",
        "The maximum impurity for a binary classification problem occurs when classes are split 50%-50%, giving\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "0.5\n",
        "Gini=0.5.\n",
        "Example Calculation:\n",
        "Suppose a node contains:\n",
        "\n",
        "4 samples of Class A (\n",
        "𝑝\n",
        "𝐴\n",
        "=\n",
        "4\n",
        "10\n",
        "=\n",
        "0.4\n",
        "p\n",
        "A\n",
        "​\n",
        " =\n",
        "10\n",
        "4\n",
        "​\n",
        " =0.4)\n",
        "6 samples of Class B (\n",
        "𝑝\n",
        "𝐵\n",
        "=\n",
        "6\n",
        "10\n",
        "=\n",
        "0.6\n",
        "p\n",
        "B\n",
        "​\n",
        " =\n",
        "10\n",
        "6\n",
        "​\n",
        " =0.6)\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.4\n",
        "2\n",
        "+\n",
        "0.6\n",
        "2\n",
        ")\n",
        "Gini=1−(0.4\n",
        "2\n",
        " +0.6\n",
        "2\n",
        " )\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.16\n",
        "+\n",
        "0.36\n",
        ")\n",
        "=\n",
        "1\n",
        "−\n",
        "0.52\n",
        "=\n",
        "0.48\n",
        "Gini=1−(0.16+0.36)=1−0.52=0.48\n",
        "Thus, the impurity of this node is 0.48."
      ],
      "metadata": {
        "id": "_GipKoLPd471"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question--4-->\n",
        "\n",
        "The mathematical formula for Entropy in a Decision Tree is:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "𝑝\n",
        "𝑖\n",
        "Entropy=−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝑐\n",
        "c is the total number of classes.\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  is the probability of a data point belonging to class\n",
        "𝑖\n",
        "i within a node.\n",
        "log\n",
        "⁡\n",
        "2\n",
        "log\n",
        "2\n",
        "​\n",
        "  is the base-2 logarithm, ensuring the entropy is measured in bits.\n",
        "Explanation:\n",
        "If a node is pure (all samples belong to one class),\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "Entropy=0 (low randomness).\n",
        "If the classes are evenly split, entropy is maximized.\n",
        "For a binary classification, the maximum entropy is 1, occurring when\n",
        "𝑝\n",
        "1\n",
        "=\n",
        "0.5\n",
        "p\n",
        "1\n",
        "​\n",
        " =0.5 and\n",
        "𝑝\n",
        "2\n",
        "=\n",
        "0.5\n",
        "p\n",
        "2\n",
        "​\n",
        " =0.5.\n",
        "Example Calculation:\n",
        "Suppose a node contains:\n",
        "\n",
        "4 samples of Class A (\n",
        "𝑝\n",
        "𝐴\n",
        "=\n",
        "4\n",
        "10\n",
        "=\n",
        "0.4\n",
        "p\n",
        "A\n",
        "​\n",
        " =\n",
        "10\n",
        "4\n",
        "​\n",
        " =0.4)\n",
        "6 samples of Class B (\n",
        "𝑝\n",
        "𝐵\n",
        "=\n",
        "6\n",
        "10\n",
        "=\n",
        "0.6\n",
        "p\n",
        "B\n",
        "​\n",
        " =\n",
        "10\n",
        "6\n",
        "​\n",
        " =0.6)\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "(\n",
        "0.4\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.4\n",
        "+\n",
        "0.6\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.6\n",
        ")\n",
        "Entropy=−(0.4log\n",
        "2\n",
        "​\n",
        " 0.4+0.6log\n",
        "2\n",
        "​\n",
        " 0.6)\n",
        "Using logarithm values:\n",
        "\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.4\n",
        "≈\n",
        "−\n",
        "1.32\n",
        ",\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.6\n",
        "≈\n",
        "−\n",
        "0.74\n",
        "log\n",
        "2\n",
        "​\n",
        " 0.4≈−1.32,log\n",
        "2\n",
        "​\n",
        " 0.6≈−0.74\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "[\n",
        "(\n",
        "0.4\n",
        "×\n",
        "−\n",
        "1.32\n",
        ")\n",
        "+\n",
        "(\n",
        "0.6\n",
        "×\n",
        "−\n",
        "0.74\n",
        ")\n",
        "]\n",
        "Entropy=−[(0.4×−1.32)+(0.6×−0.74)]\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "(\n",
        "−\n",
        "0.528\n",
        "−\n",
        "0.444\n",
        ")\n",
        "=\n",
        "0.972\n",
        "Entropy=−(−0.528−0.444)=0.972\n",
        "Thus, the entropy of this node is 0.972."
      ],
      "metadata": {
        "id": "93UxFV87eLZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question--5-->\n",
        "\n",
        "What is Information Gain?\n",
        "Information Gain (IG) is a metric used in Decision Trees to determine the best feature to split the dataset. It measures the reduction in entropy (uncertainty) after splitting the data on a specific feature.\n",
        "\n",
        "Mathematical Formula for Information Gain\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝑃\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        ")\n",
        "−\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑁\n",
        "𝑗\n",
        "𝑁\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐶\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑗\n",
        ")\n",
        "IG=Entropy(Parent)−\n",
        "j=1\n",
        "∑\n",
        "k\n",
        "​\n",
        "  \n",
        "N\n",
        "N\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        " ×Entropy(Child\n",
        "j\n",
        "​\n",
        " )\n",
        "where:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝑃\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        ")\n",
        "Entropy(Parent) = Entropy before splitting.\n",
        "𝑘\n",
        "k = Number of child nodes after the split.\n",
        "𝑁\n",
        "𝑗\n",
        "N\n",
        "j\n",
        "​\n",
        "  = Number of samples in child node\n",
        "𝑗\n",
        "j.\n",
        "𝑁\n",
        "N = Total samples in the parent node.\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐶\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑗\n",
        ")\n",
        "Entropy(Child\n",
        "j\n",
        "​\n",
        " ) = Entropy of the\n",
        "𝑗\n",
        "𝑡\n",
        "ℎ\n",
        "j\n",
        "th\n",
        "  child node.\n",
        "How is Information Gain Used in Decision Trees?\n",
        "Compute Entropy of the Parent Node before the split.\n",
        "Compute Entropy of Each Child Node after splitting using a feature.\n",
        "Calculate Information Gain: The feature with the highest IG is chosen for splitting.\n",
        "Repeat for Remaining Features recursively until a stopping condition is met (e.g., pure nodes, max depth).\n",
        "Example Calculation:\n",
        "Suppose we have 10 samples:\n",
        "\n",
        "6 belong to Class A, 4 belong to Class B.\n",
        "Entropy of the parent node:\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝑃\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        ")\n",
        "=\n",
        "−\n",
        "(\n",
        "0.6\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.6\n",
        "+\n",
        "0.4\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.4\n",
        ")\n",
        "=\n",
        "0.97\n",
        "Entropy(Parent)=−(0.6log\n",
        "2\n",
        "​\n",
        " 0.6+0.4log\n",
        "2\n",
        "​\n",
        " 0.4)=0.97\n",
        "After splitting using Feature X, we get two child nodes:\n",
        "Child 1 (4 samples: 3A, 1B) → Entropy = 0.81\n",
        "Child 2 (6 samples: 3A, 3B) → Entropy = 1.00\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "0.97\n",
        "−\n",
        "(\n",
        "4\n",
        "10\n",
        "×\n",
        "0.81\n",
        "+\n",
        "6\n",
        "10\n",
        "×\n",
        "1.00\n",
        ")\n",
        "IG=0.97−(\n",
        "10\n",
        "4\n",
        "​\n",
        " ×0.81+\n",
        "10\n",
        "6\n",
        "​\n",
        " ×1.00)\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "0.97\n",
        "−\n",
        "(\n",
        "0.324\n",
        "+\n",
        "0.6\n",
        ")\n",
        "=\n",
        "0.046\n",
        "IG=0.97−(0.324+0.6)=0.046\n",
        "Since IG is low, the split may not be useful, and we should check other features.\n",
        "\n",
        "Key Points:\n",
        "Higher IG = Better feature for splitting.\n",
        "Decision Trees prefer high IG to reduce uncertainty faster.\n",
        "ID3, C4.5, and C5.0 algorithms use Information Gain for feature selection"
      ],
      "metadata": {
        "id": "tXO_ZmRqed1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question--6-->\n",
        "\n",
        "Difference Between Gini Impurity and Entropy\n",
        "Feature\tGini Impurity\tEntropy\n",
        "Definition\tMeasures the probability of misclassifying a randomly chosen sample.\tMeasures the amount of uncertainty (randomness) in the dataset.\n",
        "Formula\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−∑p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "𝑝\n",
        "𝑖\n",
        "Entropy=−∑p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        "\n",
        "Value Range\t0 (pure node) to 0.5 (maximum impurity in binary classification).\t0 (pure node) to 1 (maximum entropy in binary classification).\n",
        "Splitting Criterion\tPrefers larger class separations (computationally faster).\tMore sensitive to changes in class distribution.\n",
        "Computation Speed\tFaster (does not involve logarithms).\tSlightly slower (uses logarithms).\n",
        "Algorithm Usage\tUsed in CART (Classification and Regression Trees).\tUsed in ID3, C4.5, and C5.0.\n",
        "Bias Towards Pure Nodes\tPrefers balanced splits, reducing misclassification.\tPrefers nodes with clear separations, reducing randomness.\n",
        "Example Comparison:\n",
        "Suppose we have a node with 10 samples:\n",
        "\n",
        "4 from Class A (\n",
        "𝑝\n",
        "𝐴\n",
        "=\n",
        "0.4\n",
        "p\n",
        "A\n",
        "​\n",
        " =0.4)\n",
        "6 from Class B (\n",
        "𝑝\n",
        "𝐵\n",
        "=\n",
        "0.6\n",
        "p\n",
        "B\n",
        "​\n",
        " =0.6)\n",
        "Gini Impurity Calculation\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.4\n",
        "2\n",
        "+\n",
        "0.6\n",
        "2\n",
        ")\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.16\n",
        "+\n",
        "0.36\n",
        ")\n",
        "=\n",
        "0.48\n",
        "Gini=1−(0.4\n",
        "2\n",
        " +0.6\n",
        "2\n",
        " )=1−(0.16+0.36)=0.48\n",
        "Entropy Calculation\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "(\n",
        "0.4\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.4\n",
        "+\n",
        "0.6\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.6\n",
        ")\n",
        "Entropy=−(0.4log\n",
        "2\n",
        "​\n",
        " 0.4+0.6log\n",
        "2\n",
        "​\n",
        " 0.6)\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "(\n",
        "0.4\n",
        "×\n",
        "−\n",
        "1.32\n",
        "+\n",
        "0.6\n",
        "×\n",
        "−\n",
        "0.74\n",
        ")\n",
        "=\n",
        "0.97\n",
        "Entropy=−(0.4×−1.32+0.6×−0.74)=0.97\n",
        "Which One to Use?\n",
        "Use Gini Impurity when computational efficiency is important (e.g., CART).\n",
        "Use Entropy when interpretability and sensitivity to class distributions matter (ID3, C4.5).\n",
        "In practice, both yield similar results, but Gini is preferred in large datasets due to speed"
      ],
      "metadata": {
        "id": "9e4kglZuevnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question--7-->\n",
        "\n",
        "Mathematical Explanation Behind Decision Trees\n",
        "A Decision Tree is a hierarchical model that makes decisions by recursively splitting the data based on feature values. The goal is to minimize impurity at each step to create the best classification or regression model.\n",
        "\n",
        "1. Splitting Criterion (Choosing the Best Feature)\n",
        "To decide the best feature for splitting, we use impurity measures like:\n",
        "\n",
        "Gini Impurity\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "Entropy (Information Gain)\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "𝑝\n",
        "𝑖\n",
        "Entropy=−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        "\n",
        "Variance Reduction (for regression trees)\n",
        "𝑉\n",
        "𝑎\n",
        "𝑟\n",
        "𝑖\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "Variance=\n",
        "N\n",
        "1\n",
        "​\n",
        " ∑(y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "ˉ\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "For classification, we compute the Information Gain (IG) for each feature:\n",
        "\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝑃\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        ")\n",
        "−\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑁\n",
        "𝑗\n",
        "𝑁\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐶\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑗\n",
        ")\n",
        "IG=Entropy(Parent)−\n",
        "j=1\n",
        "∑\n",
        "k\n",
        "​\n",
        "  \n",
        "N\n",
        "N\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        " ×Entropy(Child\n",
        "j\n",
        "​\n",
        " )\n",
        "The feature with the highest IG (or lowest Gini impurity) is chosen for splitting.\n",
        "\n",
        "2. Recursive Splitting\n",
        "The dataset is recursively split using the best feature until a stopping condition is met:\n",
        "\n",
        "All samples in a node belong to the same class (pure node,\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "0\n",
        "Gini=0).\n",
        "Maximum depth of the tree is reached.\n",
        "Minimum number of samples in a node is reached.\n",
        "Mathematically, at each step, we check:\n",
        "\n",
        "𝑆\n",
        "𝑡\n",
        "𝑜\n",
        "𝑝\n",
        "\n",
        "𝑖\n",
        "𝑓\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "0\n",
        "or\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "StopifGini=0orEntropy=0\n",
        "3. Decision Function (Making Predictions)\n",
        "Once the tree is trained, predictions for new samples are made by traversing the tree from the root to a leaf node.\n",
        "Given an input\n",
        "𝑋\n",
        "X, the model finds the path based on feature values and returns the class label (classification) or mean output (regression).\n",
        "\n",
        "For Classification:\n",
        "\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "arg\n",
        "⁡\n",
        "max\n",
        "⁡\n",
        "𝑖\n",
        "𝑝\n",
        "𝑖\n",
        "y\n",
        "^\n",
        "​\n",
        " =arg\n",
        "i\n",
        "max\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        "\n",
        "(Choose the class with the highest probability in the leaf node)\n",
        "\n",
        "For Regression:\n",
        "\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "^\n",
        "​\n",
        " =\n",
        "N\n",
        "1\n",
        "​\n",
        " ∑y\n",
        "i\n",
        "​\n",
        "\n",
        "(Take the average of all values in the leaf node)\n",
        "\n",
        "4. Pruning (Avoiding Overfitting)\n",
        "To prevent overfitting, pruning techniques are used:\n",
        "\n",
        "Pre-pruning: Stop splitting early (e.g., setting max depth).\n",
        "Post-pruning: Remove nodes that do not improve accuracy using cost complexity pruning:\n",
        "𝐶\n",
        "𝑜\n",
        "𝑠\n",
        "𝑡\n",
        "(\n",
        "𝑇\n",
        ")\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐿\n",
        "𝐸\n",
        "𝑖\n",
        "+\n",
        "𝛼\n",
        "∣\n",
        "𝑇\n",
        "∣\n",
        "Cost(T)=\n",
        "i=1\n",
        "∑\n",
        "L\n",
        "​\n",
        " E\n",
        "i\n",
        "​\n",
        " +α∣T∣\n",
        "where\n",
        "𝐸\n",
        "𝑖\n",
        "E\n",
        "i\n",
        "​\n",
        "  is the error at each leaf node, and\n",
        "𝛼\n",
        "α is a regularization parameter.\n",
        "Summary of Mathematical Process\n",
        "Compute impurity (Gini, Entropy, Variance) at each split.\n",
        "Calculate Information Gain to choose the best feature.\n",
        "Recursively split until a stopping condition is met.\n",
        "Make predictions by traversing the tree.\n",
        "Apply pruning to avoid overfitting."
      ],
      "metadata": {
        "id": "Oe_hhDQHMy0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question--8-->\n",
        "\n",
        "Pre-Pruning in Decision Trees\n",
        "Pre-Pruning (also known as early stopping) is a technique used to stop the tree from growing too deep by applying constraints before it fully develops. This helps to prevent overfitting and improves generalization.\n",
        "\n",
        "How Does Pre-Pruning Work?\n",
        "While building the Decision Tree, a stopping condition is checked at each step. If the condition is met, the splitting process stops, and the current node becomes a leaf.\n",
        "\n",
        "Common Pre-Pruning Techniques:\n",
        "Maximum Depth (max_depth)\n",
        "\n",
        "Limits how deep the tree can grow.\n",
        "Prevents excessive branching and reduces complexity.\n",
        "Mathematically: Stop if\n",
        "𝑑\n",
        "𝑒\n",
        "𝑝\n",
        "𝑡\n",
        "ℎ\n",
        "(\n",
        "𝑛\n",
        "𝑜\n",
        "𝑑\n",
        "𝑒\n",
        ")\n",
        "≥\n",
        "𝑚\n",
        "𝑎\n",
        "𝑥\n",
        "_\n",
        "𝑑\n",
        "𝑒\n",
        "𝑝\n",
        "𝑡\n",
        "ℎ\n",
        "depth(node)≥max_depth\n",
        "Minimum Samples per Split (min_samples_split)\n",
        "\n",
        "Ensures that a node must have at least\n",
        "𝑛\n",
        "n samples to be split further.\n",
        "If a node has fewer samples, it becomes a leaf.\n",
        "Mathematically: Stop if\n",
        "∣\n",
        "𝑆\n",
        "∣\n",
        "<\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "_\n",
        "𝑠\n",
        "𝑎\n",
        "𝑚\n",
        "𝑝\n",
        "𝑙\n",
        "𝑒\n",
        "𝑠\n",
        "_\n",
        "𝑠\n",
        "𝑝\n",
        "𝑙\n",
        "𝑖\n",
        "𝑡\n",
        "∣S∣<min_samples_split\n",
        "where\n",
        "𝑆\n",
        "S is the number of samples in the node.\n",
        "Minimum Samples per Leaf (min_samples_leaf)\n",
        "\n",
        "Specifies the minimum number of samples a leaf node must contain.\n",
        "Helps prevent small, unreliable leaf nodes.\n",
        "Mathematically: Stop if\n",
        "∣\n",
        "𝑆\n",
        "𝑙\n",
        "𝑒\n",
        "𝑎\n",
        "𝑓\n",
        "∣\n",
        "<\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "_\n",
        "𝑠\n",
        "𝑎\n",
        "𝑚\n",
        "𝑝\n",
        "𝑙\n",
        "𝑒\n",
        "𝑠\n",
        "_\n",
        "𝑙\n",
        "𝑒\n",
        "𝑎\n",
        "𝑓\n",
        "∣S\n",
        "leaf\n",
        "​\n",
        " ∣<min_samples_leaf\n",
        "Maximum Number of Nodes (max_leaf_nodes)\n",
        "\n",
        "Limits the total number of leaf nodes in the tree.\n",
        "The tree grows until it reaches the limit.\n",
        "Minimum Information Gain (min_impurity_decrease)\n",
        "\n",
        "Stops splitting when the reduction in impurity is below a threshold.\n",
        "Mathematically: Stop if\n",
        "𝐼\n",
        "𝐺\n",
        "<\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "_\n",
        "𝑖\n",
        "𝑚\n",
        "𝑝\n",
        "𝑢\n",
        "𝑟\n",
        "𝑖\n",
        "𝑡\n",
        "𝑦\n",
        "_\n",
        "𝑑\n",
        "𝑒\n",
        "𝑐\n",
        "𝑟\n",
        "𝑒\n",
        "𝑎\n",
        "𝑠\n",
        "𝑒\n",
        "IG<min_impurity_decrease\n",
        "where\n",
        "𝐼\n",
        "𝐺\n",
        "IG is the Information Gain.\n",
        "Advantages of Pre-Pruning\n",
        "✅ Reduces overfitting by controlling tree growth.\n",
        "✅ Improves model interpretability (simpler trees).\n",
        "✅ Reduces training time and computational cost.\n",
        "\n",
        "Disadvantages\n",
        "❌ Risk of underfitting if the stopping criteria are too strict.\n",
        "❌ Requires tuning of parameters like max_depth and min_samples_leaf.\n",
        "\n",
        "Example: Pre-Pruning in Python (Scikit-Learn)\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Creating a Decision Tree with pre-pruning\n",
        "clf = DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "EKrXaOS7NFsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question--9-->\n",
        "\n",
        "Post-Pruning in Decision Trees\n",
        "Post-Pruning (also called cost-complexity pruning) is a technique applied after building a full decision tree. It removes unnecessary branches to reduce overfitting and improve generalization.\n",
        "\n",
        "How Does Post-Pruning Work?\n",
        "Grow the tree fully: Let the decision tree expand until all nodes are pure (or meet a minimal stopping criterion).\n",
        "Prune the tree from bottom to top: Remove nodes that do not improve accuracy significantly.\n",
        "Evaluate the pruned tree: Use cross-validation to ensure that pruning helps generalization.\n",
        "Techniques for Post-Pruning\n",
        "Reduced Error Pruning\n",
        "\n",
        "Start from the leaf nodes and remove them if the error does not increase on validation data.\n",
        "Stop when further pruning increases misclassification.\n",
        "Simple but less effective compared to cost-complexity pruning.\n",
        "Cost-Complexity Pruning (CCP) (used in Scikit-Learn)\n",
        "\n",
        "Introduces a penalty term to balance tree complexity and accuracy.\n",
        "Uses a parameter\n",
        "𝛼\n",
        "α to control pruning:\n",
        "𝐶\n",
        "𝑜\n",
        "𝑠\n",
        "𝑡\n",
        "(\n",
        "𝑇\n",
        ")\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐿\n",
        "𝐸\n",
        "𝑖\n",
        "+\n",
        "𝛼\n",
        "∣\n",
        "𝑇\n",
        "∣\n",
        "Cost(T)=\n",
        "i=1\n",
        "∑\n",
        "L\n",
        "​\n",
        " E\n",
        "i\n",
        "​\n",
        " +α∣T∣\n",
        "where:\n",
        "𝐸\n",
        "𝑖\n",
        "E\n",
        "i\n",
        "​\n",
        "  = error at each leaf node,\n",
        "∣\n",
        "𝑇\n",
        "∣\n",
        "∣T∣ = number of leaf nodes,\n",
        "𝛼\n",
        "α = complexity parameter (higher\n",
        "𝛼\n",
        "α → more pruning).\n",
        "Advantages of Post-Pruning\n",
        "✅ Reduces overfitting while keeping important branches.\n",
        "✅ Often results in better accuracy compared to pre-pruning.\n",
        "✅ Works well with cross-validation to find optimal pruning levels.\n",
        "\n",
        "Disadvantages\n",
        "❌ Computationally expensive, as it requires building the full tree first.\n",
        "❌ If pruned too aggressively, it can lead to underfitting.\n",
        "\n",
        "Example: Post-Pruning in Python (Scikit-Learn)\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Step 1: Train an unpruned decision tree\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 2: Get cost-complexity pruning path\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas  # List of alpha values for pruning\n",
        "\n",
        "# Step 3: Train pruned trees for different alpha values\n",
        "pruned_trees = [DecisionTreeClassifier(ccp_alpha=alpha).fit(X_train, y_train) for alpha in ccp_alphas]\n",
        "\n",
        "# Step 4: Choose the best tree using validation data"
      ],
      "metadata": {
        "id": "5nQeprdxNTlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question--10-->\n",
        "\n",
        "Difference Between Pre-Pruning and Post-Pruning\n",
        "Feature\tPre-Pruning (Early Stopping)\tPost-Pruning (Cost-Complexity Pruning)\n",
        "Timing\tApplied before the tree is fully grown.\tApplied after the tree is fully grown.\n",
        "How It Works\tStops further splitting based on constraints like max_depth, min_samples_split, etc.\tGrows the tree completely, then removes unnecessary branches.\n",
        "Criteria Used\t- Maximum depth (max_depth)\n",
        "- Minimum samples per split (min_samples_split)\n",
        "- Minimum impurity decrease (min_impurity_decrease)\t- Cost-complexity pruning (ccp_alpha)\n",
        "- Reduced error pruning (removes nodes if accuracy does not decrease).\n",
        "Computational Cost\tFaster (stops early, reducing training time).\tSlower (builds full tree first, then prunes).\n",
        "Overfitting Control\tPrevents overfitting early by limiting growth.\tReduces overfitting after seeing how the tree performs.\n",
        "Underfitting Risk\tHigher (may stop too soon and miss useful splits).\tLower (removes only unnecessary nodes).\n",
        "Flexibility\tLess flexible, as it relies on fixed thresholds.\tMore flexible, as pruning decisions are based on actual tree performance.\n",
        "Usage in Algorithms\tCommon in CART, ID3, and C4.5.\tCommon in CART (with cost-complexity pruning).\n",
        "Which One to Use?\n",
        "Use Pre-Pruning when computational efficiency is a priority and when you want to prevent excessive tree growth.\n",
        "Use Post-Pruning when you want to allow full tree growth and then refine it based on validation performance.\n",
        "Example in Python (Scikit-Learn)\n",
        "Pre-Pruning:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Applying pre-pruning with max depth\n",
        "clf = DecisionTreeClassifier(max_depth=5, min_samples_split=10)\n",
        "clf.fit(X_train, y_train)\n",
        "Post-Pruning (Cost-Complexity Pruning):\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Get cost complexity pruning path\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas  \n",
        "\n",
        "# Train multiple pruned trees\n",
        "pruned_trees = [DecisionTreeClassifier(ccp_alpha=alpha).fit(X_train, y_train) for alpha in ccp_alpha"
      ],
      "metadata": {
        "id": "xjdYs71lNjtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question--11-->\n",
        "\n",
        "Decision Tree Regressor\n",
        "A Decision Tree Regressor is a type of supervised learning algorithm used for regression tasks. Instead of classifying data into categories (like in classification trees), it predicts continuous numerical values by recursively splitting the dataset based on feature values.\n",
        "\n",
        "How Does a Decision Tree Regressor Work?\n",
        "Recursive Splitting:\n",
        "\n",
        "The dataset is split into smaller subsets based on feature values to minimize error.\n",
        "The goal is to reduce variance within each subset.\n",
        "Splitting Criterion (Minimizing Variance):\n",
        "\n",
        "Unlike classification trees that use Gini Impurity or Entropy, regression trees use Mean Squared Error (MSE) or Mean Absolute Error (MAE) to decide splits.\n",
        "The best split is chosen to minimize the variance:\n",
        "𝑀\n",
        "𝑆\n",
        "𝐸\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∑\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "MSE=\n",
        "n\n",
        "1\n",
        "​\n",
        " ∑(y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "ˉ\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "where\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "  are the target values, and\n",
        "𝑦\n",
        "ˉ\n",
        "y\n",
        "ˉ\n",
        "​\n",
        "  is the mean of the node.\n",
        "Prediction:\n",
        "\n",
        "After training, for a given input\n",
        "𝑋\n",
        "X, the model follows the decision path down the tree and outputs the mean target value of the final leaf node.\n",
        "Mathematical Representation\n",
        "At each node, the best split is chosen by minimizing MSE:\n",
        "\n",
        "MSE\n",
        "split\n",
        "=\n",
        "𝑁\n",
        "𝐿\n",
        "𝑁\n",
        "𝑀\n",
        "𝑆\n",
        "𝐸\n",
        "𝐿\n",
        "+\n",
        "𝑁\n",
        "𝑅\n",
        "𝑁\n",
        "𝑀\n",
        "𝑆\n",
        "𝐸\n",
        "𝑅\n",
        "MSE\n",
        "split\n",
        "​\n",
        " =\n",
        "N\n",
        "N\n",
        "L\n",
        "​\n",
        "\n",
        "​\n",
        " MSE\n",
        "L\n",
        "​\n",
        " +\n",
        "N\n",
        "N\n",
        "R\n",
        "​\n",
        "\n",
        "​\n",
        " MSE\n",
        "R\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝑁\n",
        "𝐿\n",
        "N\n",
        "L\n",
        "​\n",
        " ,\n",
        "𝑁\n",
        "𝑅\n",
        "N\n",
        "R\n",
        "​\n",
        "  = Number of samples in left and right nodes\n",
        "𝑀\n",
        "𝑆\n",
        "𝐸\n",
        "𝐿\n",
        "MSE\n",
        "L\n",
        "​\n",
        " ,\n",
        "𝑀\n",
        "𝑆\n",
        "𝐸\n",
        "𝑅\n",
        "MSE\n",
        "R\n",
        "​\n",
        "  = Mean squared error of left and right nodes\n",
        "The tree grows until it meets stopping criteria like max_depth, min_samples_split, or min_impurity_decrease.\n",
        "\n",
        "Advantages of Decision Tree Regression\n",
        "✅ Interpretable & easy to visualize\n",
        "✅ Handles non-linearity well\n",
        "✅ Requires little data preprocessing (no need for feature scaling)\n",
        "✅ Can capture feature interactions automatically\n",
        "\n",
        "Disadvantages\n",
        "❌ Prone to overfitting (requires pruning or regularization)\n",
        "❌ Not smooth (creates step-like predictions instead of continuous curves)\n",
        "❌ Sensitive to small changes (small variations in data can change the tree structure)\n",
        "\n",
        "Example: Decision Tree Regressor in Python (Scikit-Learn)\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate sample data\n",
        "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])  # Sin wave with noise\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(max_depth=3)\n",
        "regressor.fit(X, y)\n",
        "\n",
        "# Predict values\n",
        "X_test = np.linspace(0, 5, 100).reshape(-1, 1)\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Plot results\n",
        "plt.scatter(X, y, label=\"Actual data\")\n",
        "plt.plot(X_test, y_pred, color='red', label=\"Predicted regression line\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "This will show a step-like regression plot based on the decision tree splits.\n",
        "\n",
        "When to Use a Decision Tree Regressor?\n",
        "✔ When the data has non-linear relationships that linear regression cannot capture.\n",
        "✔ When interpretability and rule-based decision-making are important.\n",
        "✔ When dealing with small to medium-sized datasets (not too high-dimensional)."
      ],
      "metadata": {
        "id": "Sbe7Xrx7NzpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question--12-->\n",
        "\n",
        "Advantages and Disadvantages of Decision Trees\n",
        "✅ Advantages\n",
        "Easy to Understand & Interpret\n",
        "\n",
        "Decision trees are intuitive and can be easily visualized.\n",
        "The decision-making process mimics human reasoning.\n",
        "No Need for Feature Scaling\n",
        "\n",
        "Unlike models like SVM or linear regression, decision trees do not require normalization or standardization.\n",
        "Handles Both Numerical & Categorical Data\n",
        "\n",
        "Works well with both continuous (regression) and categorical (classification) variables.\n",
        "Captures Non-Linearity & Feature Interactions\n",
        "\n",
        "Decision trees can automatically capture complex relationships without needing manual feature engineering.\n",
        "Handles Missing Values\n",
        "\n",
        "Unlike many ML models, decision trees can handle missing data by using surrogate splits.\n",
        "Efficient on Small to Medium Datasets\n",
        "\n",
        "Works well when the dataset is not too large or high-dimensional.\n",
        "Fast Training & Prediction\n",
        "\n",
        "Training and inference are computationally efficient, especially with simple trees.\n",
        "❌ Disadvantages\n",
        "Prone to Overfitting\n",
        "\n",
        "A fully grown tree can become too complex, memorizing the training data instead of generalizing.\n",
        "Solution: Use pruning (pre-pruning, post-pruning) or ensemble methods like Random Forest.\n",
        "Sensitive to Small Changes in Data\n",
        "\n",
        "A small change in training data can completely alter the tree structure (high variance).\n",
        "Solution: Use ensemble methods like Bagging or Random Forest.\n",
        "Biased Towards Dominant Classes\n",
        "\n",
        "If one class has significantly more samples than others, the tree may become biased.\n",
        "Solution: Balance the dataset before training.\n",
        "Not Ideal for High-Dimensional Data\n",
        "\n",
        "With too many features, trees become deep and complex, leading to overfitting.\n",
        "Solution: Use feature selection techniques before training.\n",
        "Non-Smooth Predictions (for Regression Trees)\n",
        "\n",
        "Decision Tree Regression produces step-like predictions instead of smooth curves.\n",
        "Solution: Use Random Forest or Gradient Boosting for smoother outputs.\n",
        "Comparison with Other Models\n",
        "Model\tInterpretability\tHandles Non-Linearity\tOverfitting Risk\tRequires Scaling\n",
        "Decision Tree\t✅ High\t✅ Yes\t❌ High (without pruning)\t✅ No\n",
        "Linear Regression\t✅ High\t❌ No (only linear)\t✅ Low\t✅ Yes\n",
        "Random Forest\t❌ Lower\t✅ Yes\t✅ Lower\t✅ No\n",
        "Neural Networks\t❌ Low\t✅ Yes\t❌ High\t✅ Yes"
      ],
      "metadata": {
        "id": "Qyfr8nMXOCfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question--13-->\n",
        "\n",
        "How Does a Decision Tree Handle Missing Values?\n",
        "Decision Trees can naturally handle missing values in several ways. The approach depends on the implementation and whether the missing values are in features (independent variables) or target labels (dependent variable).\n",
        "\n",
        "1️⃣ Handling Missing Values in Features (Input Data)\n",
        "(A) Surrogate Splitting (Used in CART Algorithm)\n",
        "If a feature has missing values at a node, the tree finds an alternative feature (surrogate split) that best mimics the original decision.\n",
        "If the primary splitting feature is missing, the model uses the next best feature.\n",
        "This ensures that data points with missing values can still be classified.\n",
        "🔹 Example:\n",
        "\n",
        "Suppose the tree chooses \"Age\" to split, but some data points have missing age values.\n",
        "It finds a \"similar\" feature, like \"Income\", that gives a similar split and uses it as a backup.\n",
        "(B) Assigning Missing Values to the Most Common Category (Mode/Median Imputation)\n",
        "For categorical features, the missing value is replaced with the most common category in that node.\n",
        "For numerical features, it is replaced with the median value of that node.\n",
        "🔹 Example:\n",
        "\n",
        "If the Gender feature is missing in some records, the tree assigns the most frequent gender (e.g., \"Male\").\n",
        "(C) Distributing Probabilities Instead of Hard Assignments\n",
        "Some implementations (like C4.5 Algorithm) assign missing values based on the probabilities of known values.\n",
        "Instead of assigning a single value, missing data is split into different branches proportionally.\n",
        "🔹 Example:\n",
        "\n",
        "If 80% of data goes left and 20% goes right, a missing value will follow the split with 80% probability to the left and 20% to the right.\n",
        "2️⃣ Handling Missing Values in the Target Variable (Output Data)\n",
        "If the target variable (label) is missing, the decision tree simply ignores those samples during training.\n",
        "However, if too many labels are missing, it's better to use imputation techniques (like kNN or mean imputation) before training.\n",
        "Practical Example: Handling Missing Values in Scikit-Learn\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset with missing values\n",
        "X = np.array([[25, 50000], [30, np.nan], [35, 70000], [np.nan, 60000]])\n",
        "y = np.array([0, 1, 0, 1])  # Binary classification\n",
        "\n",
        "# Impute missing values using median strategy\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Train Decision Tree\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_imputed, y)\n",
        "Which Decision Tree Algorithms Handle Missing Values Best?\n",
        "Algorithm\tHandles Missing Features?\tUses Surrogate Splits?\tProbabilistic Assignment?\n",
        "CART (Scikit-Learn)\t✅ Yes\t✅ Yes\t❌ No\n",
        "ID3\t❌ No\t❌ No\t❌ No\n",
        "C4.5\t✅ Yes\t❌ No\t✅ Yes\n",
        "XGBoost / Random Forest\t✅ Yes\t✅ Yes\t❌ No"
      ],
      "metadata": {
        "id": "W-TLKpN4OKkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question--14-->\n",
        "\n",
        "How Does a Decision Tree Handle Categorical Features?\n",
        "Decision Trees can naturally handle categorical features without requiring one-hot encoding. The way categorical features are processed depends on the algorithm used (e.g., ID3, C4.5, CART).\n",
        "\n",
        "1️⃣ Splitting Categorical Features in Decision Trees\n",
        "(A) One-vs-All (Binary Splits for Each Category) – Used in CART\n",
        "The tree chooses one category and creates a binary split (e.g., \"Is Color = Red? Yes/No\").\n",
        "Works well when the number of categories is small.\n",
        "Example:\n",
        "mathematica\n",
        "Copy\n",
        "Edit\n",
        "If Color = Red → Left node\n",
        "Else → Right node\n",
        "Limitation: Not ideal for high-cardinality categorical features (many unique values).\n",
        "(B) Multi-Way Splitting (One Branch Per Category) – Used in ID3 & C4.5\n",
        "The tree creates multiple child nodes, each representing a different category.\n",
        "Example:\n",
        "mathematica\n",
        "Copy\n",
        "Edit\n",
        "      Color?\n",
        "      /  |  \\\n",
        "    Red  Blue  Green\n",
        "Limitation: Can lead to a very bushy tree if there are too many categories.\n",
        "(C) Encoding Categorical Features Numerically\n",
        "Some decision trees require categorical data to be converted into numbers before splitting:\n",
        "\n",
        "Label Encoding (Ordinal Encoding)\n",
        "\n",
        "Assigns a unique integer to each category.\n",
        "Works well for ordinal categories (e.g., \"Low\" = 0, \"Medium\" = 1, \"High\" = 2).\n",
        "Limitation: Misinterprets categories as having an order when there is none.\n",
        "One-Hot Encoding (OHE)\n",
        "\n",
        "Converts categories into binary vectors (e.g., \"Red\" → [1,0,0], \"Blue\" → [0,1,0]).\n",
        "Limitation: Increases dimensionality when there are many unique categories.\n",
        "2️⃣ Which Splitting Criteria Are Used for Categorical Features?\n",
        "For categorical variables, decision trees use:\n",
        "\n",
        "✅ Gini Impurity:\n",
        "\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−∑p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "✅ Entropy (ID3, C4.5):\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−∑p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "✅ Chi-Square Test (CHAID Algorithm):\n",
        "\n",
        "Measures statistical significance between categorical splits.\n",
        "3️⃣ Example: Handling Categorical Features in Scikit-Learn\n",
        "Method 1: Using Label Encoding for DecisionTreeClassifier\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sample dataset with categorical feature\n",
        "X = [[\"Red\"], [\"Blue\"], [\"Green\"], [\"Red\"], [\"Blue\"]]\n",
        "y = [0, 1, 0, 1, 0]  # Target labels\n",
        "\n",
        "# Convert categories to numbers\n",
        "encoder = LabelEncoder()\n",
        "X_encoded = encoder.fit_transform([x[0] for x in X]).reshape(-1, 1)\n",
        "\n",
        "# Train Decision Tree\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_encoded, y)\n",
        "Method 2: Using One-Hot Encoding\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# One-hot encode categorical feature\n",
        "encoder = OneHotEncoder()\n",
        "X_encoded = encoder.fit_transform(X).toarray()\n",
        "\n",
        "# Train Decision Tree\n",
        "clf.fit(X_encoded, y)\n",
        "4️⃣ Which Decision Tree Algorithms Handle Categorical Features Best?\n",
        "Algorithm\tHandles Categorical Features Directly?\tUses One-Hot Encoding?\n",
        "CART (Scikit-Learn)\t❌ No (Requires Encoding)\t✅ Yes\n",
        "ID3 / C4.5\t✅ Yes (Multi-way Split)\t❌ No\n",
        "XGBoost\t✅ Yes (Can handle categorical features natively)\t❌ No\n",
        "CatBoost\t✅ Best for Categorical Data\t❌ No"
      ],
      "metadata": {
        "id": "KoEvohEdOaX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question--15-->\n",
        "\n",
        "Real-World Applications of Decision Trees 🌍🌳\n",
        "Decision Trees are widely used in various domains due to their simplicity, interpretability, and ability to handle both numerical and categorical data. Here are some practical applications:\n",
        "\n",
        "1️⃣ Healthcare & Medical Diagnosis 🏥\n",
        "✅ Disease Prediction & Diagnosis\n",
        "\n",
        "Decision trees help in diagnosing diseases like diabetes, cancer, and heart disease.\n",
        "Example: Predicting whether a patient has COVID-19 based on symptoms like fever, cough, and fatigue.\n",
        "✅ Treatment Recommendation\n",
        "\n",
        "Used to recommend treatments based on a patient’s medical history and symptoms.\n",
        "🔹 Example:\n",
        "A decision tree could classify a patient as low-risk or high-risk for a disease based on blood pressure, cholesterol levels, and smoking habits.\n",
        "\n",
        "2️⃣ Banking & Finance 💰\n",
        "✅ Credit Scoring & Loan Approval\n",
        "\n",
        "Banks use decision trees to evaluate whether a person is eligible for a loan or credit card based on income, credit history, and debt-to-income ratio.\n",
        "✅ Fraud Detection\n",
        "\n",
        "Identifies fraudulent transactions by analyzing spending patterns and detecting anomalies.\n",
        "🔹 Example:\n",
        "A bank may deny a loan if a decision tree finds that the applicant has high debt, low income, and a bad credit score.\n",
        "\n",
        "3️⃣ E-Commerce & Retail 🛒\n",
        "✅ Customer Segmentation & Recommendation Systems\n",
        "\n",
        "Decision Trees help in segmenting customers based on purchasing behavior.\n",
        "Used to recommend products based on browsing history and past purchases.\n",
        "✅ Churn Prediction\n",
        "\n",
        "Predicts whether a customer will stop using a service based on engagement levels.\n",
        "🔹 Example:\n",
        "Amazon or Flipkart can use a decision tree to decide whether to offer a discount based on factors like purchase frequency, cart abandonment, and customer loyalty.\n",
        "\n",
        "4️⃣ Manufacturing & Quality Control 🏭\n",
        "✅ Defect Detection in Products\n",
        "\n",
        "Decision Trees help manufacturers identify faulty products based on inspection data.\n",
        "✅ Supply Chain Optimization\n",
        "\n",
        "Used to determine the optimal inventory levels and predict demand.\n",
        "🔹 Example:\n",
        "A car manufacturer may use a decision tree to determine which parts are most likely to fail based on production defects.\n",
        "\n",
        "5️⃣ Marketing & Advertisement 📢\n",
        "✅ Targeted Advertising\n",
        "\n",
        "Decision Trees help companies personalize ads based on customer demographics and behavior.\n",
        "✅ Lead Scoring\n",
        "\n",
        "Identifies high-value potential customers for sales teams.\n",
        "🔹 Example:\n",
        "Google Ads can use a decision tree to decide which ad to show based on factors like user interests, browsing history, and location.\n",
        "\n",
        "6️⃣ Education & E-Learning 🎓\n",
        "✅ Student Performance Prediction\n",
        "\n",
        "Schools and universities use decision trees to predict student success or risk of dropping out.\n",
        "✅ Personalized Learning Paths\n",
        "\n",
        "Adaptive learning platforms use decision trees to suggest courses and study materials based on student progress.\n",
        "🔹 Example:\n",
        "An AI-driven education platform can recommend more practice problems for students struggling in mathematics.\n",
        "\n"
      ],
      "metadata": {
        "id": "fY0GRHoiOl8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#practical\n",
        "\n",
        "#Question 16--\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf05w2vOOzZZ",
        "outputId": "4ff8a2e5-266a-40cc-9b7d-85c757772399"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question--17\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = pd.DataFrame(\n",
        "    {\"Feature\": iris.feature_names, \"Importance\": clf.feature_importances_}\n",
        ").sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"Feature Importances:\\n\", feature_importances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UgCBK84PRZC",
        "outputId": "4f6355aa-01eb-48c3-f87b-9a31cdf0d4d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "              Feature  Importance\n",
            "2  petal length (cm)    0.906143\n",
            "3   petal width (cm)    0.077186\n",
            "1   sepal width (cm)    0.016670\n",
            "0  sepal length (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# question--18\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier using Entropy\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Model Accuracy (Entropy): {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoxgInrlPVct",
        "outputId": "eed817bb-f1f6-451d-f566-87acf41f9a24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Model Accuracy (Entropy): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question--19\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier using Entropy\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Model Accuracy (Entropy): {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bweRU7GRPmbZ",
        "outputId": "a9ad77e0-ec2e-45c9-86a7-1142c361686c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Model Accuracy (Entropy): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question-19\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data  # Features (e.g., MedInc, HouseAge, etc.)\n",
        "y = housing.target  # Target (Median House Value)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(criterion=\"squared_error\", random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Decision Tree Regressor MSE: {mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogoRVX9VPqhi",
        "outputId": "004d0a95-c5dc-4339-e6ce-742c7b6c8b11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor MSE: 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question--20\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Export tree structure to Graphviz format\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,  # No need to write to a file\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,  # Add colors\n",
        "    rounded=True,  # Rounded nodes\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "# Render the tree using Graphviz\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"decision_tree\")  # Saves as 'decision_tree.pdf'\n",
        "graph.view()  # Opens the visualization\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WLYh5yVyQF7R",
        "outputId": "33d9617e-9dee-4a8b-ed12-cc482ace67e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'decision_tree.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question--21\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Train fully grown Decision Tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with fully grown tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZMTC6gAQtrk",
        "outputId": "04709851-d21d-4b9d-e528-5122d0110ea1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.00\n",
            "Accuracy with fully grown tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question--22\n",
        "\n",
        "# Train Decision Tree with min_samples_split=5\n",
        "clf_limited = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Train default Decision Tree\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "clf_default.fit(X_train, y_train)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "print(f\"Accuracy with min_samples_split=5: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with default tree: {accuracy_default:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT3Hr-cERKJn",
        "outputId": "e611d570-c58d-42bd-952f-de3ab5125dba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with min_samples_split=5: 1.00\n",
            "Accuracy with default tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question--23\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Decision Tree on unscaled data\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "accuracy_unscaled = accuracy_score(y_test, clf_unscaled.predict(X_test))\n",
        "\n",
        "# Train Decision Tree on scaled data\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "accuracy_scaled = accuracy_score(y_test, clf_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(f\"Accuracy without scaling: {accuracy_unscaled:.2f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajLpEqv3RQ02",
        "outputId": "7aaf1e84-d025-493a-b333-e7cd5d00d7b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 1.00\n",
            "Accuracy with scaling: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question--24\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Train a Decision Tree with One-vs-Rest strategy\n",
        "ovr_clf = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "accuracy_ovr = accuracy_score(y_test, ovr_clf.predict(X_test))\n",
        "\n",
        "print(f\"Accuracy with One-vs-Rest (OvR): {accuracy_ovr:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvDuTSYqRWJK",
        "outputId": "c01c4f2b-58e9-45ec-92e1-747bfbc8b82c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with One-vs-Rest (OvR): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question--25\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Train the Decision Tree\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = pd.DataFrame(\n",
        "    {\"Feature\": iris.feature_names, \"Importance\": clf.feature_importances_}\n",
        ").sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"Feature Importances:\\n\", feature_importances)\n"
      ],
      "metadata": {
        "id": "y96He42WRcFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#questiion--26\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor with max_depth=5\n",
        "regressor_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "regressor_limited.fit(X_train, y_train)\n",
        "y_pred_limited = regressor_limited.predict(X_test)\n",
        "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
        "\n",
        "# Train fully grown Decision Tree Regressor\n",
        "regressor_full = DecisionTreeRegressor(random_state=42)\n",
        "regressor_full.fit(X_train, y_train)\n",
        "y_pred_full = regressor_full.predict(X_test)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "\n",
        "print(f\"MSE with max_depth=5: {mse_limited:.2f}\")\n",
        "print(f\"MSE with fully grown tree: {mse_full:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeoElA8DRhx-",
        "outputId": "34be7161-f133-4bdc-9f2e-a43ee07db023"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE with max_depth=5: 0.52\n",
            "MSE with fully grown tree: 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question--26\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree without pruning to find cost complexity pruning path\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas[:-1]  # Exclude maximum alpha\n",
        "\n",
        "# Train trees with different alpha values\n",
        "accuracies = []\n",
        "for alpha in ccp_alphas:\n",
        "    clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    clf_pruned.fit(X_train, y_train)\n",
        "    y_pred = clf_pruned.predict(X_test)\n",
        "    accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Plot CCP vs Accuracy\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ccp_alphas, accuracies, marker=\"o\", linestyle=\"--\")\n",
        "plt.xlabel(\"Cost Complexity Pruning Alpha\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of CCP on Accuracy\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "4_UbfkOcRxCE",
        "outputId": "e99a4397-ec1f-4646-dc44-ca6e461e4850"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question--27\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate using Precision, Recall, and F1-Score\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEbeaDTYR3G5",
        "outputId": "3a273c55-f102-4993-d81a-39c54bfbf5f0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question--28\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate using Precision, Recall, and F1-Score\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvqARs94R8vM",
        "outputId": "cc9a9a25-48cb-4afa-a92c-cb5123599a09"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question--29\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot using seaborn\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "NCUUx3q_SDoS",
        "outputId": "f371c88c-437b-4e26-f561-81bc9a2fd587"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question--30\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    \"max_depth\": [3, 5, 10, None],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best hyperparameters\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "print(f\"Optimized Decision Tree Accuracy: {accuracy_best:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzXYFuoqSX3J",
        "outputId": "9771d1fe-6d2d-4d5e-81c4-f6f51f0eed47"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 10, 'min_samples_split': 2}\n",
            "Optimized Decision Tree Accuracy: 1.00\n"
          ]
        }
      ]
    }
  ]
}